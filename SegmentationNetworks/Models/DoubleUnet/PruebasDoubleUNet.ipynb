{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJvy-WaJATZ-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFnNVIfrDsYV"
   },
   "source": [
    "modelo: (`model.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oijR1kwZDr6e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torchvision.transforms.functional as TF\n",
    "\n",
    "# Bloque de código de cada capa de la Unet (donde se aplican las dos convoluciones, Además se aplica el btchnorm):\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "\n",
    "        # Down part of Unet:\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of Unet:\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottlenech = DoubleConv(features[-1], features[-1]*2) # Medium part\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1) #Final part\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottlenech(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((3, 1, 161, 161))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "    print(x.shape)\n",
    "    assert preds.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvK6_Ui7D4q2",
    "outputId": "cd5f9f49-67cf-4776-ff20-a5762595169f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 161, 161])\n",
      "torch.Size([3, 1, 161, 161])\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvThhiKXJSbJ"
   },
   "source": [
    "Cargar el zip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FL4esknI_N2-",
    "outputId": "6c758a97-83a2-4c3e-be17-704cb1212319"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4daac9b08de1>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model2_weights.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# CARGAR EL MODELO DEL ZIP (solo para guardar este código):\n",
    "# prompt: Ahora dame el código con el que lo cargaría en el otro script, asignándolo a la variable model2\n",
    "import torch\n",
    "# from model import UNET  # Assuming your UNET class is in a file named model.py\n",
    "import zipfile\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_from_zip(zip_filename, device=\"cpu\"):\n",
    "  \"\"\"Loads a PyTorch model from a zip file.\n",
    "\n",
    "  Args:\n",
    "    zip_filename: The name of the zip file containing the model weights.\n",
    "    device: The device to load the model onto (e.g., \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    The loaded PyTorch model.\n",
    "  \"\"\"\n",
    "\n",
    "  with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
    "    zipf.extractall()\n",
    "\n",
    "  model = UNET(in_channels=3, out_channels=1).to(device)\n",
    "  model.load_state_dict(torch.load(\"model2_weights.pth\", map_location=device))\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "# Example of how to load the model\n",
    "model2 = load_model_from_zip(\"model2.zip\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CTbToYSr_hMk"
   },
   "outputs": [],
   "source": [
    "# prompt: ahora calculame el accuracy y el coeficiente dice del modelo entrenado con ayuda de las carpetas /content/FootSegmentation_4/images-unet/IMs29Ago/test_images y /content/FootSegmentation_4/images-unet/IMs29Ago/test_masks\n",
    "\n",
    "def calculate_metrics(test_image_dir, test_mask_dir, model, device=\"cuda\", image_height=240, image_width=240):\n",
    "  \"\"\"Calculates accuracy and Dice coefficient for a trained model on a test dataset.\n",
    "\n",
    "  Args:\n",
    "    test_image_dir: Path to the directory containing test images.\n",
    "    test_mask_dir: Path to the directory containing test masks.\n",
    "    model: The trained PyTorch model.\n",
    "    device: The device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "    image_height: The height to resize images to before feeding them to the model.\n",
    "    image_width: The width to resize images to before feeding them to the model.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the accuracy and Dice coefficient.\n",
    "  \"\"\"\n",
    "\n",
    "  num_correct = 0\n",
    "  num_pixels = 0\n",
    "  dice_score = 0\n",
    "  model.eval()\n",
    "\n",
    "  val_transforms = A.Compose(\n",
    "      [\n",
    "          A.Resize(height=image_height, width=image_width),\n",
    "          A.Normalize(\n",
    "              mean=[0.0, 0.0, 0.0],\n",
    "              std=[1.0, 1.0, 1.0],\n",
    "              max_pixel_value=255.0,\n",
    "          ),\n",
    "          ToTensorV2(),\n",
    "      ]\n",
    "  )\n",
    "\n",
    "\n",
    "  image_files = [f for f in os.listdir(test_image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "  for image_file in image_files:\n",
    "      img_path = os.path.join(test_image_dir, image_file)\n",
    "      mask_path = os.path.join(test_mask_dir, image_file)\n",
    "      image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "      mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "      mask[mask == 255.0] = 1.0\n",
    "\n",
    "      augmentations = val_transforms(image=image, mask=mask)\n",
    "      image = augmentations[\"image\"]\n",
    "      mask = augmentations[\"mask\"]\n",
    "\n",
    "\n",
    "      x = image.unsqueeze(0).to(device)\n",
    "      y = torch.tensor(mask).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "          preds = torch.sigmoid(model(x))\n",
    "          preds = (preds > 0.5).float()\n",
    "          num_correct += (preds == y).sum()\n",
    "          num_pixels += torch.numel(preds)\n",
    "          dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
    "\n",
    "  accuracy = num_correct / num_pixels if num_pixels > 0 else 0\n",
    "  dice_coefficient = dice_score / len(image_files) if len(image_files) > 0 else 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  return accuracy, dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGQJTIY7_1GR",
    "outputId": "719e79b4-a4e5-431f-c018-a64f17db76d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-aaa2fed69dac>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(mask).unsqueeze(0).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9972\n",
      "Dice Coefficient: 0.7957\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# test_image_dir = \"/content/FootSegmentation_4/images-unet/IMs29Ago/test_images\"\n",
    "# test_mask_dir = \"/content/FootSegmentation_4/images-unet/IMs29Ago/test_masks\"\n",
    "test_image_dir = \"/content/wound-segmentation/data/Foot Ulcer Segmentation Challenge/validation/images\"\n",
    "test_mask_dir = \"/content/wound-segmentation/data/Foot Ulcer Segmentation Challenge/validation/labels\"\n",
    "\n",
    "accuracy, dice_coefficient = calculate_metrics(test_image_dir, test_mask_dir, model2, device=DEVICE)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Dice Coefficient: {dice_coefficient:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-vpJ_CGKAeI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
