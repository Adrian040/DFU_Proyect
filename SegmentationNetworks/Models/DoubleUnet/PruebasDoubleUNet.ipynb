{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TJvy-WaJATZ-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\am969\\anaconda3\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFnNVIfrDsYV"
   },
   "source": [
    "modelo: (`model.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oijR1kwZDr6e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torchvision.transforms.functional as TF\n",
    "\n",
    "# Bloque de código de cada capa de la Unet (donde se aplican las dos convoluciones, Además se aplica el btchnorm):\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "\n",
    "        # Down part of Unet:\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of Unet:\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottlenech = DoubleConv(features[-1], features[-1]*2) # Medium part\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1) #Final part\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottlenech(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((3, 1, 161, 161))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "    print(x.shape)\n",
    "    assert preds.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvK6_Ui7D4q2",
    "outputId": "cd5f9f49-67cf-4776-ff20-a5762595169f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 161, 161])\n",
      "torch.Size([3, 1, 161, 161])\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvThhiKXJSbJ"
   },
   "source": [
    "Cargar el zip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FL4esknI_N2-",
    "outputId": "6c758a97-83a2-4c3e-be17-704cb1212319"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\am969\\AppData\\Local\\Temp\\ipykernel_19104\\3887677613.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model2_weights.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# CARGAR EL MODELO DEL ZIP (solo para guardar este código):\n",
    "# prompt: Ahora dame el código con el que lo cargaría en el otro script, asignándolo a la variable model2\n",
    "# import torch\n",
    "# from model import UNET  # Assuming your UNET class is in a file named model.py\n",
    "import zipfile\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_from_zip(zip_filename, device=\"cpu\"):\n",
    "  \"\"\"Loads a PyTorch model from a zip file.\n",
    "\n",
    "  Args:\n",
    "    zip_filename: The name of the zip file containing the model weights.\n",
    "    device: The device to load the model onto (e.g., \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    The loaded PyTorch model.\n",
    "  \"\"\"\n",
    "\n",
    "  with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
    "    zipf.extractall()\n",
    "\n",
    "  model = UNET(in_channels=3, out_channels=1).to(device)\n",
    "  model.load_state_dict(torch.load(\"model2_weights.pth\", map_location=device))\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "# Example of how to load the model\n",
    "model2 = load_model_from_zip(\"C:/Users/am969/Documents/DFU_Proyect/SegmentationNetworks/trained_Models/Models_Zps_Imgs/model_061124/model2.zip\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CTbToYSr_hMk"
   },
   "outputs": [],
   "source": [
    "# prompt: ahora calculame el accuracy y el coeficiente dice del modelo entrenado con ayuda de las carpetas /content/FootSegmentation_4/images-unet/IMs29Ago/test_images y /content/FootSegmentation_4/images-unet/IMs29Ago/test_masks\n",
    "\n",
    "def calculate_metrics(test_image_dir, test_mask_dir, model, device=\"cuda\", image_height=224, image_width=224):\n",
    "  \"\"\"Calculates accuracy and Dice coefficient for a trained model on a test dataset.\n",
    "\n",
    "  Args:\n",
    "    test_image_dir: Path to the directory containing test images.\n",
    "    test_mask_dir: Path to the directory containing test masks.\n",
    "    model: The trained PyTorch model.\n",
    "    device: The device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
    "    image_height: The height to resize images to before feeding them to the model.\n",
    "    image_width: The width to resize images to before feeding them to the model.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the accuracy and Dice coefficient.\n",
    "  \"\"\"\n",
    "\n",
    "  num_correct = 0\n",
    "  num_pixels = 0\n",
    "  dice_score = 0\n",
    "  model.eval()\n",
    "\n",
    "  val_transforms = A.Compose(\n",
    "      [\n",
    "          A.Resize(height=image_height, width=image_width),\n",
    "          A.Normalize(\n",
    "              mean=[0.0, 0.0, 0.0],\n",
    "              std=[1.0, 1.0, 1.0],\n",
    "              max_pixel_value=255.0,\n",
    "          ),\n",
    "          ToTensorV2(),\n",
    "      ]\n",
    "  )\n",
    "\n",
    "\n",
    "  image_files = [f for f in os.listdir(test_image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "  for image_file in image_files:\n",
    "      img_path = os.path.join(test_image_dir, image_file)\n",
    "      mask_path = os.path.join(test_mask_dir, image_file)\n",
    "      image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "      mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "      mask[mask == 255.0] = 1.0\n",
    "\n",
    "      augmentations = val_transforms(image=image, mask=mask)\n",
    "      image = augmentations[\"image\"]\n",
    "      mask = augmentations[\"mask\"]\n",
    "\n",
    "\n",
    "      x = image.unsqueeze(0).to(device)\n",
    "      y = torch.tensor(mask).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "          preds = torch.sigmoid(model(x))\n",
    "          preds = (preds > 0.5).float()\n",
    "          num_correct += (preds == y).sum()\n",
    "          num_pixels += torch.numel(preds)\n",
    "          dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
    "\n",
    "  accuracy = num_correct / num_pixels if num_pixels > 0 else 0\n",
    "  dice_coefficient = dice_score / len(image_files) if len(image_files) > 0 else 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  return accuracy, dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGQJTIY7_1GR",
    "outputId": "719e79b4-a4e5-431f-c018-a64f17db76d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\am969\\AppData\\Local\\Temp\\ipykernel_19104\\2375844145.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(mask).unsqueeze(0).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9453\n",
      "Dice Coefficient: 0.5446\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# test_image_dir = \"/content/FootSegmentation_4/images-unet/IMs29Ago/test_images\"\n",
    "# test_mask_dir = \"/content/FootSegmentation_4/images-unet/IMs29Ago/test_masks\"\n",
    "test_image_dir = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/images/\"\n",
    "test_mask_dir = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/true_masks/\"\n",
    "\n",
    "accuracy, dice_coefficient = calculate_metrics(test_image_dir, test_mask_dir, model2, device=DEVICE)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Dice Coefficient: {dice_coefficient:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recortar las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "R-vpJ_CGKAeI"
   },
   "outputs": [],
   "source": [
    "# prompt: Dame una función que tome como argumentos: el path de una carpeta de imágenes, así como el de una carpeta con sus respectivas máscaras binarias y que me cree otras dos carpetas (una de imágenes y otra de máscaras) de las imágnes y máscaras recortadas en un 5% del contorno.\n",
    "\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "def crop_images_and_masks(image_dir, mask_dir, output_image_dir, output_mask_dir, crop_percentage=0.05):\n",
    "    \"\"\"\n",
    "    Crops images and their corresponding masks by a given percentage.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Path to the directory containing images.\n",
    "        mask_dir: Path to the directory containing masks.\n",
    "        output_image_dir: Path to the output directory for cropped images.\n",
    "        output_mask_dir: Path to the output directory for cropped masks.\n",
    "        crop_percentage: The percentage to crop from each edge (default: 5%).\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_image_dir):\n",
    "        os.makedirs(output_image_dir)\n",
    "    if not os.path.exists(output_mask_dir):\n",
    "        os.makedirs(output_mask_dir)\n",
    "\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            mask_path = os.path.join(mask_dir, filename)  # Assuming same filenames\n",
    "\n",
    "            try:\n",
    "                image = Image.open(image_path)\n",
    "                mask = Image.open(mask_path)\n",
    "\n",
    "                # Get image dimensions\n",
    "                width, height = image.size\n",
    "\n",
    "                # Calculate cropping amounts\n",
    "                crop_width = int(width * crop_percentage)\n",
    "                crop_height = int(height * crop_percentage)\n",
    "\n",
    "                # Crop the image and mask\n",
    "                cropped_image = image.crop((crop_width, crop_height, width - crop_width, height - crop_height))\n",
    "                cropped_mask = mask.crop((crop_width, crop_height, width - crop_width, height - crop_height))\n",
    "\n",
    "                # Save the cropped image and mask\n",
    "                cropped_image_path = os.path.join(output_image_dir, filename)\n",
    "                cropped_mask_path = os.path.join(output_mask_dir, filename)\n",
    "                cropped_image.save(cropped_image_path)\n",
    "                cropped_mask.save(cropped_mask_path)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                print(f\"Mask file not found for {filename}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "image_folder = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/images/\"  # Replace with your image folder path\n",
    "mask_folder = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/true_masks/\"      # Replace with your mask folder path\n",
    "output_image_folder = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/croped_images\"  # Output folder for cropped images\n",
    "output_mask_folder = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/croped_masks\"   # Output folder for cropped masks\n",
    "\n",
    "crop_images_and_masks(image_folder, mask_folder, output_image_folder, output_mask_folder, crop_percentage=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos con las imágenes recortadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\am969\\AppData\\Local\\Temp\\ipykernel_19104\\2375844145.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(mask).unsqueeze(0).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9385\n",
      "Dice Coefficient: 0.5422\n"
     ]
    }
   ],
   "source": [
    "test_image_dir = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/croped_images/\"\n",
    "test_mask_dir = \"C:/Users/am969/Documents/DFU_Proyect/data_DFU_images/Images_Gerardo/testing_12/testing-images/croped_masks/\"\n",
    "\n",
    "accuracy, dice_coefficient = calculate_metrics(test_image_dir, test_mask_dir, model2, device=DEVICE)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Dice Coefficient: {dice_coefficient:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
